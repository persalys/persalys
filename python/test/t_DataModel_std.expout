class=DataModel name=myDataModel fileName=data.csv inputColumns=[0,2] outputColumns=[] inputNames=[X0,X2] outputNames=[]
inputNames= [X0,X2]
outputNames= []
result= class=DataAnalysisResult name=Unnamed min=[[-2.29006],[-1.31178]] max=[[1.78236],[0.810668]] mean=[[0.201254],[-0.143151]] median=[[0.459701],[0.0607826]] standardDeviation=[[1.12247],[0.678845]] coefficientOfVariation=[[5.57739],[4.74217]] variance=[[1.25994],[0.460831]] skewness=[[-1.06322],[-0.430218]] kurtosis=[[5.03838],[2.07116]] firstQuartile=[[-0.355007],[-0.781366]] thirdQuartile=[[0.793156],[0.350042]] meanConfidenceInterval=class=Interval name=Unnamed dimension=2 lower bound=class=Point name=Unnamed dimension=2 values=[-0.494448,-0.563896] upper bound=class=Point name=Unnamed dimension=2 values=[0.896956,0.277594] finite lower bound=[1,1] finite upper bound=[1,1] stdConfidenceInterval=class=Interval name=Unnamed dimension=2 lower bound=class=Point name=Unnamed dimension=2 values=[0.772075,0.466934] upper bound=class=Point name=Unnamed dimension=2 values=[2.0492,1.23931] finite lower bound=[1,1] finite upper bound=[1,1]
PDF= [  0 : [ -4.21702      0.000969752 ]
  1 : [ -4.14787      0.00129605  ]
  2 : [ -4.07872      0.00171387  ]
  3 : [ -4.00958      0.00224252  ]
  4 : [ -3.94043      0.00290331  ]
  5 : [ -3.87128      0.0037192   ]
  6 : [ -3.80213      0.0047142   ]
  7 : [ -3.73298      0.00591246  ]
  8 : [ -3.66384      0.00733722  ]
  9 : [ -3.59469      0.00900949  ]
 10 : [ -3.52554      0.0109465   ]
 11 : [ -3.45639      0.0131602   ]
 12 : [ -3.38724      0.0156555   ]
 13 : [ -3.3181       0.0184285   ]
 14 : [ -3.24895      0.0214655   ]
 15 : [ -3.1798       0.0247418   ]
 16 : [ -3.11065      0.0282209   ]
 17 : [ -3.0415       0.0318551   ]
 18 : [ -2.97236      0.0355858   ]
 19 : [ -2.90321      0.0393454   ]
 20 : [ -2.83406      0.0430597   ]
 21 : [ -2.76491      0.0466506   ]
 22 : [ -2.69577      0.0500405   ]
 23 : [ -2.62662      0.0531562   ]
 24 : [ -2.55747      0.0559332   ]
 25 : [ -2.48832      0.0583206   ]
 26 : [ -2.41917      0.0602853   ]
 27 : [ -2.35003      0.0618151   ]
 28 : [ -2.28088      0.0629222   ]
 29 : [ -2.21173      0.0636445   ]
 30 : [ -2.14258      0.064046    ]
 31 : [ -2.07343      0.0642165   ]
 32 : [ -2.00429      0.0642688   ]
 33 : [ -1.93514      0.0643356   ]
 34 : [ -1.86599      0.0645649   ]
 35 : [ -1.79684      0.0651145   ]
 36 : [ -1.72769      0.0661455   ]
 37 : [ -1.65855      0.0678162   ]
 38 : [ -1.5894       0.0702751   ]
 39 : [ -1.52025      0.0736549   ]
 40 : [ -1.4511       0.078067    ]
 41 : [ -1.38196      0.0835962   ]
 42 : [ -1.31281      0.0902976   ]
 43 : [ -1.24366      0.0981941   ]
 44 : [ -1.17451      0.107276    ]
 45 : [ -1.10536      0.117499    ]
 46 : [ -1.03622      0.128791    ]
 47 : [ -0.967068     0.141051    ]
 48 : [ -0.89792      0.154153    ]
 49 : [ -0.828772     0.167955    ]
 50 : [ -0.759624     0.182299    ]
 51 : [ -0.690476     0.197019    ]
 52 : [ -0.621328     0.211944    ]
 53 : [ -0.55218      0.226906    ]
 54 : [ -0.483032     0.241737    ]
 55 : [ -0.413885     0.256277    ]
 56 : [ -0.344737     0.270374    ]
 57 : [ -0.275589     0.28388     ]
 58 : [ -0.206441     0.296659    ]
 59 : [ -0.137293     0.30858     ]
 60 : [ -0.068145     0.319518    ]
 61 : [  0.00100292   0.329356    ]
 62 : [  0.0701508    0.337981    ]
 63 : [  0.139299     0.345289    ]
 64 : [  0.208447     0.351184    ]
 65 : [  0.277595     0.355581    ]
 66 : [  0.346742     0.358408    ]
 67 : [  0.41589      0.359613    ]
 68 : [  0.485038     0.359162    ]
 69 : [  0.554186     0.357048    ]
 70 : [  0.623334     0.353288    ]
 71 : [  0.692482     0.347927    ]
 72 : [  0.76163      0.341039    ]
 73 : [  0.830778     0.332723    ]
 74 : [  0.899926     0.323103    ]
 75 : [  0.969074     0.312321    ]
 76 : [  1.03822      0.300535    ]
 77 : [  1.10737      0.287911    ]
 78 : [  1.17652      0.274619    ]
 79 : [  1.24567      0.260829    ]
 80 : [  1.31481      0.246702    ]
 81 : [  1.38396      0.232388    ]
 82 : [  1.45311      0.218025    ]
 83 : [  1.52226      0.203735    ]
 84 : [  1.5914       0.189623    ]
 85 : [  1.66055      0.175779    ]
 86 : [  1.7297       0.162279    ]
 87 : [  1.79885      0.149184    ]
 88 : [  1.868        0.136547    ]
 89 : [  1.93714      0.124409    ]
 90 : [  2.00629      0.112807    ]
 91 : [  2.07544      0.101769    ]
 92 : [  2.14459      0.091321    ]
 93 : [  2.21374      0.0814834   ]
 94 : [  2.28288      0.072273    ]
 95 : [  2.35203      0.0637022   ]
 96 : [  2.42118      0.0557787   ]
 97 : [  2.49033      0.0485046   ]
 98 : [  2.55948      0.0418763   ]
 99 : [  2.62862      0.0358841   ]
100 : [  2.69777      0.0305117   ]
101 : [  2.76692      0.0257365   ]
102 : [  2.83607      0.0215304   ]
103 : [  2.90521      0.0178597   ]
104 : [  2.97436      0.014687    ]
105 : [  3.04351      0.0119714   ]
106 : [  3.11266      0.00967031  ]
107 : [  3.18181      0.00774016  ]
108 : [  3.25095      0.00613782  ]
109 : [  3.3201       0.00482146  ]
110 : [  3.38925      0.00375141  ]
111 : [  3.4584       0.00289079  ]
112 : [  3.52755      0.002206    ]
113 : [  3.59669      0.00166695  ]
114 : [  3.66584      0.00124721  ]
115 : [  3.73499      0.000923902 ]
116 : [  3.80414      0.00067757  ]
117 : [  3.87329      0.000491928 ]
118 : [  3.94243      0.000353546 ]
119 : [  4.01158      0.000251518 ]
120 : [  4.08073      0.000177115 ]
121 : [  4.14988      0.000123449 ]
122 : [  4.21903      8.51626e-05 ]
123 : [  4.28817      5.81476e-05 ]
124 : [  4.35732      3.92936e-05 ]
125 : [  4.42647      2.62791e-05 ]
126 : [  4.49562      1.73935e-05 ]
127 : [  4.56476      1.13932e-05 ]
128 : [  4.63391      7.38542e-06 ],  0 : [ -3.16918      1.81964e-06 ]
  1 : [ -3.12246      3.13174e-06 ]
  2 : [ -3.07575      5.31638e-06 ]
  3 : [ -3.02903      8.90191e-06 ]
  4 : [ -2.98232      1.47025e-05 ]
  5 : [ -2.9356       2.39524e-05 ]
  6 : [ -2.88889      3.84913e-05 ]
  7 : [ -2.84217      6.10154e-05 ]
  8 : [ -2.79546      9.54092e-05 ]
  9 : [ -2.74874      0.000147173 ]
 10 : [ -2.70203      0.000223956 ]
 11 : [ -2.65531      0.000336209 ]
 12 : [ -2.60859      0.000497953 ]
 13 : [ -2.56188      0.00072764  ]
 14 : [ -2.51516      0.0010491   ]
 15 : [ -2.46845      0.00149251  ]
 16 : [ -2.42173      0.00209528  ]
 17 : [ -2.37502      0.00290289  ]
 18 : [ -2.3283       0.00396934  ]
 19 : [ -2.28159      0.00535733  ]
 20 : [ -2.23487      0.00713793  ]
 21 : [ -2.18816      0.00938955  ]
 22 : [ -2.14144      0.0121964   ]
 23 : [ -2.09472      0.0156458   ]
 24 : [ -2.04801      0.0198258   ]
 25 : [ -2.00129      0.0248208   ]
 26 : [ -1.95458      0.0307081   ]
 27 : [ -1.90786      0.0375536   ]
 28 : [ -1.86115      0.0454079   ]
 29 : [ -1.81443      0.0543027   ]
 30 : [ -1.76772      0.0642484   ]
 31 : [ -1.721        0.0752324   ]
 32 : [ -1.67429      0.0872179   ]
 33 : [ -1.62757      0.100145    ]
 34 : [ -1.58086      0.113932    ]
 35 : [ -1.53414      0.128478    ]
 36 : [ -1.48742      0.143662    ]
 37 : [ -1.44071      0.159353    ]
 38 : [ -1.39399      0.175403    ]
 39 : [ -1.34728      0.191658    ]
 40 : [ -1.30056      0.207955    ]
 41 : [ -1.25385      0.224125    ]
 42 : [ -1.20713      0.239995    ]
 43 : [ -1.16042      0.255393    ]
 44 : [ -1.1137       0.270149    ]
 45 : [ -1.06699      0.284101    ]
 46 : [ -1.02027      0.297102    ]
 47 : [ -0.973555     0.309026    ]
 48 : [ -0.926839     0.319778    ]
 49 : [ -0.880124     0.3293      ]
 50 : [ -0.833408     0.337582    ]
 51 : [ -0.786693     0.344668    ]
 52 : [ -0.739977     0.350655    ]
 53 : [ -0.693262     0.355702    ]
 54 : [ -0.646547     0.360022    ]
 55 : [ -0.599831     0.363875    ]
 56 : [ -0.553116     0.367559    ]
 57 : [ -0.5064       0.371393    ]
 58 : [ -0.459685     0.3757      ]
 59 : [ -0.412969     0.380786    ]
 60 : [ -0.366254     0.386916    ]
 61 : [ -0.319539     0.394296    ]
 62 : [ -0.272823     0.403047    ]
 63 : [ -0.226108     0.413192    ]
 64 : [ -0.179392     0.424639    ]
 65 : [ -0.132677     0.437179    ]
 66 : [ -0.0859615    0.450485    ]
 67 : [ -0.0392461    0.464119    ]
 68 : [  0.00746936   0.477554    ]
 69 : [  0.0541848    0.490193    ]
 70 : [  0.1009       0.501407    ]
 71 : [  0.147616     0.510566    ]
 72 : [  0.194331     0.51708     ]
 73 : [  0.241046     0.520431    ]
 74 : [  0.287762     0.52021     ]
 75 : [  0.334477     0.51614     ]
 76 : [  0.381193     0.508092    ]
 77 : [  0.427908     0.496093    ]
 78 : [  0.474624     0.480317    ]
 79 : [  0.521339     0.461079    ]
 80 : [  0.568054     0.438804    ]
 81 : [  0.61477      0.414006    ]
 82 : [  0.661485     0.387253    ]
 83 : [  0.708201     0.359137    ]
 84 : [  0.754916     0.330245    ]
 85 : [  0.801632     0.301136    ]
 86 : [  0.848347     0.272316    ]
 87 : [  0.895062     0.244229    ]
 88 : [  0.941778     0.217246    ]
 89 : [  0.988493     0.191665    ]
 90 : [  1.03521      0.167709    ]
 91 : [  1.08192      0.145533    ]
 92 : [  1.12864      0.12523     ]
 93 : [  1.17536      0.106839    ]
 94 : [  1.22207      0.0903525   ]
 95 : [  1.26879      0.0757257   ]
 96 : [  1.3155       0.062882    ]
 97 : [  1.36222      0.0517213   ]
 98 : [  1.40893      0.0421258   ]
 99 : [  1.45565      0.033965    ]
100 : [  1.50236      0.0271014   ]
101 : [  1.54908      0.0213944   ]
102 : [  1.59579      0.0167046   ]
103 : [  1.64251      0.0128967   ]
104 : [  1.68922      0.00984279  ]
105 : [  1.73594      0.00742422  ]
106 : [  1.78266      0.00553326  ]
107 : [  1.82937      0.00407398  ]
108 : [  1.87609      0.00296269  ]
109 : [  1.9228       0.00212769  ]
110 : [  1.96952      0.00150877  ]
111 : [  2.01623      0.00105625  ]
112 : [  2.06295      0.000729947 ]
113 : [  2.10966      0.000497907 ]
114 : [  2.15638      0.000335194 ]
115 : [  2.20309      0.00022269  ]
116 : [  2.24981      0.000145991 ]
117 : [  2.29653      9.4438e-05  ]
118 : [  2.34324      6.02751e-05 ]
119 : [  2.38996      3.79556e-05 ]
120 : [  2.43667      2.358e-05   ]
121 : [  2.48339      1.44519e-05 ]
122 : [  2.5301       8.73779e-06 ]
123 : [  2.57682      5.21152e-06 ]
124 : [  2.62353      3.0662e-06  ]
125 : [  2.67025      1.77951e-06 ]
126 : [  2.71696      1.01873e-06 ]
127 : [  2.76368      5.75258e-07 ]
128 : [  2.81039      3.20412e-07 ]]
CDF= [  0 : [ -4.21702      0.00020604  ]
  1 : [ -4.14787      0.000283902 ]
  2 : [ -4.07872      0.000387386 ]
  3 : [ -4.00958      0.000523476 ]
  4 : [ -3.94043      0.000700561 ]
  5 : [ -3.87128      0.000928566 ]
  6 : [ -3.80213      0.00121904  ]
  7 : [ -3.73298      0.00158521  ]
  8 : [ -3.66384      0.00204193  ]
  9 : [ -3.59469      0.00260562  ]
 10 : [ -3.52554      0.00329401  ]
 11 : [ -3.45639      0.00412586  ]
 12 : [ -3.38724      0.00512051  ]
 13 : [ -3.3181       0.00629736  ]
 14 : [ -3.24895      0.00767519  ]
 15 : [ -3.1798       0.00927147  ]
 16 : [ -3.11065      0.0111016   ]
 17 : [ -3.0415       0.0131779   ]
 18 : [ -2.97236      0.0155092   ]
 19 : [ -2.90321      0.0180999   ]
 20 : [ -2.83406      0.0209495   ]
 21 : [ -2.76491      0.0240521   ]
 22 : [ -2.69577      0.0273964   ]
 23 : [ -2.62662      0.0309661   ]
 24 : [ -2.55747      0.0347399   ]
 25 : [ -2.48832      0.0386924   ]
 26 : [ -2.41917      0.0427956   ]
 27 : [ -2.35003      0.0470196   ]
 28 : [ -2.28088      0.0513346   ]
 29 : [ -2.21173      0.0557126   ]
 30 : [ -2.14258      0.060129    ]
 31 : [ -2.07343      0.0645646   ]
 32 : [ -2.00429      0.0690071   ]
 33 : [ -1.93514      0.073453    ]
 34 : [ -1.86599      0.0779082   ]
 35 : [ -1.79684      0.0823894   ]
 36 : [ -1.72769      0.0869244   ]
 37 : [ -1.65855      0.0915518   ]
 38 : [ -1.5894       0.0963212   ]
 39 : [ -1.52025      0.101292    ]
 40 : [ -1.4511       0.106531    ]
 41 : [ -1.38196      0.112114    ]
 42 : [ -1.31281      0.118119    ]
 43 : [ -1.24366      0.124629    ]
 44 : [ -1.17451      0.131726    ]
 45 : [ -1.10536      0.139491    ]
 46 : [ -1.03622      0.148001    ]
 47 : [ -0.967068     0.157325    ]
 48 : [ -0.89792      0.167527    ]
 49 : [ -0.828772     0.17866     ]
 50 : [ -0.759624     0.190767    ]
 51 : [ -0.690476     0.20388     ]
 52 : [ -0.621328     0.218019    ]
 53 : [ -0.55218      0.233192    ]
 54 : [ -0.483032     0.249396    ]
 55 : [ -0.413885     0.266616    ]
 56 : [ -0.344737     0.284828    ]
 57 : [ -0.275589     0.303994    ]
 58 : [ -0.206441     0.32407     ]
 59 : [ -0.137293     0.345001    ]
 60 : [ -0.068145     0.366723    ]
 61 : [  0.00100292   0.389164    ]
 62 : [  0.0701508    0.412243    ]
 63 : [  0.139299     0.435875    ]
 64 : [  0.208447     0.459963    ]
 65 : [  0.277595     0.484407    ]
 66 : [  0.346742     0.509102    ]
 67 : [  0.41589      0.533936    ]
 68 : [  0.485038     0.558797    ]
 69 : [  0.554186     0.583569    ]
 70 : [  0.623334     0.608137    ]
 71 : [  0.692482     0.63239     ]
 72 : [  0.76163      0.656219    ]
 73 : [  0.830778     0.679521    ]
 74 : [  0.899926     0.702203    ]
 75 : [  0.969074     0.724178    ]
 76 : [  1.03822      0.745372    ]
 77 : [  1.10737      0.765722    ]
 78 : [  1.17652      0.785174    ]
 79 : [  1.24567      0.803689    ]
 80 : [  1.31481      0.821238    ]
 81 : [  1.38396      0.837802    ]
 82 : [  1.45311      0.853375    ]
 83 : [  1.52226      0.867956    ]
 84 : [  1.5914       0.881555    ]
 85 : [  1.66055      0.894186    ]
 86 : [  1.7297       0.905872    ]
 87 : [  1.79885      0.916638    ]
 88 : [  1.868        0.926514    ]
 89 : [  1.93714      0.935533    ]
 90 : [  2.00629      0.943732    ]
 91 : [  2.07544      0.951147    ]
 92 : [  2.14459      0.95782     ]
 93 : [  2.21374      0.963791    ]
 94 : [  2.28288      0.969103    ]
 95 : [  2.35203      0.9738      ]
 96 : [  2.42118      0.977928    ]
 97 : [  2.49033      0.981529    ]
 98 : [  2.55948      0.98465     ]
 99 : [  2.62862      0.987335    ]
100 : [  2.69777      0.989627    ]
101 : [  2.76692      0.991569    ]
102 : [  2.83607      0.9932      ]
103 : [  2.90521      0.994559    ]
104 : [  2.97436      0.995681    ]
105 : [  3.04351      0.9966      ]
106 : [  3.11266      0.997346    ]
107 : [  3.18181      0.997946    ]
108 : [  3.25095      0.998424    ]
109 : [  3.3201       0.998802    ]
110 : [  3.38925      0.999097    ]
111 : [  3.4584       0.999325    ]
112 : [  3.52755      0.999501    ]
113 : [  3.59669      0.999634    ]
114 : [  3.66584      0.999734    ]
115 : [  3.73499      0.999808    ]
116 : [  3.80414      0.999863    ]
117 : [  3.87329      0.999903    ]
118 : [  3.94243      0.999932    ]
119 : [  4.01158      0.999953    ]
120 : [  4.08073      0.999968    ]
121 : [  4.14988      0.999978    ]
122 : [  4.21903      0.999985    ]
123 : [  4.28817      0.99999     ]
124 : [  4.35732      0.999994    ]
125 : [  4.42647      0.999996    ]
126 : [  4.49562      0.999997    ]
127 : [  4.56476      0.999998    ]
128 : [  4.63391      0.999999    ],  0 : [ -3.16918      1.48359e-07 ]
  1 : [ -3.12246      2.6138e-07  ]
  2 : [ -3.07575      4.54449e-07 ]
  3 : [ -3.02903      7.79763e-07 ]
  4 : [ -2.98232      1.32044e-06 ]
  5 : [ -2.9356       2.20683e-06 ]
  6 : [ -2.88889      3.64025e-06 ]
  7 : [ -2.84217      5.92684e-06 ]
  8 : [ -2.79546      9.52497e-06 ]
  9 : [ -2.74874      1.51104e-05 ]
 10 : [ -2.70203      2.36636e-05 ]
 11 : [ -2.65531      3.65852e-05 ]
 12 : [ -2.60859      5.58443e-05 ]
 13 : [ -2.56188      8.41646e-05 ]
 14 : [ -2.51516      0.000125254 ]
 15 : [ -2.46845      0.000184078 ]
 16 : [ -2.42173      0.000267178 ]
 17 : [ -2.37502      0.000383027 ]
 18 : [ -2.3283       0.000542423 ]
 19 : [ -2.28159      0.000758889 ]
 20 : [ -2.23487      0.00104907  ]
 21 : [ -2.18816      0.00143312  ]
 22 : [ -2.14144      0.00193499  ]
 23 : [ -2.09472      0.00258265  ]
 24 : [ -2.04801      0.00340817  ]
 25 : [ -2.00129      0.00444768  ]
 26 : [ -1.95458      0.0057411   ]
 27 : [ -1.90786      0.0073317   ]
 28 : [ -1.86115      0.00926549  ]
 29 : [ -1.81443      0.0115904   ]
 30 : [ -1.76772      0.0143554   ]
 31 : [ -1.721        0.0176094   ]
 32 : [ -1.67429      0.0214      ]
 33 : [ -1.62757      0.0257729   ]
 34 : [ -1.58086      0.0307701   ]
 35 : [ -1.53414      0.0364295   ]
 36 : [ -1.48742      0.0427838   ]
 37 : [ -1.44071      0.0498599   ]
 38 : [ -1.39399      0.0576779   ]
 39 : [ -1.34728      0.0662511   ]
 40 : [ -1.30056      0.0755853   ]
 41 : [ -1.25385      0.0856785   ]
 42 : [ -1.20713      0.0965208   ]
 43 : [ -1.16042      0.108094    ]
 44 : [ -1.1137       0.120372    ]
 45 : [ -1.06699      0.133322    ]
 46 : [ -1.02027      0.146901    ]
 47 : [ -0.973555     0.161064    ]
 48 : [ -0.926839     0.175756    ]
 49 : [ -0.880124     0.190922    ]
 50 : [ -0.833408     0.206503    ]
 51 : [ -0.786693     0.222444    ]
 52 : [ -0.739977     0.238689    ]
 53 : [ -0.693262     0.255191    ]
 54 : [ -0.646547     0.271911    ]
 55 : [ -0.599831     0.288821    ]
 56 : [ -0.553116     0.305905    ]
 57 : [ -0.5064       0.323164    ]
 58 : [ -0.459685     0.340612    ]
 59 : [ -0.412969     0.358279    ]
 60 : [ -0.366254     0.376206    ]
 61 : [ -0.319539     0.394448    ]
 62 : [ -0.272823     0.413067    ]
 63 : [ -0.226108     0.432127    ]
 64 : [ -0.179392     0.451692    ]
 65 : [ -0.132677     0.471818    ]
 66 : [ -0.0859615    0.49255     ]
 67 : [ -0.0392461    0.513913    ]
 68 : [  0.00746936   0.53591     ]
 69 : [  0.0541848    0.558518    ]
 70 : [  0.1009       0.581687    ]
 71 : [  0.147616     0.605333    ]
 72 : [  0.194331     0.629348    ]
 73 : [  0.241046     0.653595    ]
 74 : [  0.287762     0.677917    ]
 75 : [  0.334477     0.702139    ]
 76 : [  0.381193     0.726078    ]
 77 : [  0.427908     0.749549    ]
 78 : [  0.474624     0.77237     ]
 79 : [  0.521339     0.794371    ]
 80 : [  0.568054     0.815401    ]
 81 : [  0.61477      0.83533     ]
 82 : [  0.661485     0.854052    ]
 83 : [  0.708201     0.87149     ]
 84 : [  0.754916     0.887594    ]
 85 : [  0.801632     0.902342    ]
 86 : [  0.848347     0.915734    ]
 87 : [  0.895062     0.927796    ]
 88 : [  0.941778     0.93857     ]
 89 : [  0.988493     0.948115    ]
 90 : [  1.03521      0.956503    ]
 91 : [  1.08192      0.963812    ]
 92 : [  1.12864      0.970129    ]
 93 : [  1.17536      0.975542    ]
 94 : [  1.22207      0.980141    ]
 95 : [  1.26879      0.984013    ]
 96 : [  1.3155       0.987244    ]
 97 : [  1.36222      0.989914    ]
 98 : [  1.40893      0.9921      ]
 99 : [  1.45565      0.993872    ]
100 : [  1.50236      0.995294    ]
101 : [  1.54908      0.996423    ]
102 : [  1.59579      0.997309    ]
103 : [  1.64251      0.997997    ]
104 : [  1.68922      0.998525    ]
105 : [  1.73594      0.998926    ]
106 : [  1.78266      0.999227    ]
107 : [  1.82937      0.99945     ]
108 : [  1.87609      0.999613    ]
109 : [  1.9228       0.999731    ]
110 : [  1.96952      0.999815    ]
111 : [  2.01623      0.999875    ]
112 : [  2.06295      0.999916    ]
113 : [  2.10966      0.999944    ]
114 : [  2.15638      0.999964    ]
115 : [  2.20309      0.999977    ]
116 : [  2.24981      0.999985    ]
117 : [  2.29653      0.999991    ]
118 : [  2.34324      0.999994    ]
119 : [  2.38996      0.999996    ]
120 : [  2.43667      0.999998    ]
121 : [  2.48339      0.999999    ]
122 : [  2.5301       0.999999    ]
123 : [  2.57682      1           ]
124 : [  2.62353      1           ]
125 : [  2.67025      1           ]
126 : [  2.71696      1           ]
127 : [  2.76368      1           ]
128 : [  2.81039      1           ]]
SurvivalFunction= [  0 : [ -4.21702      0.999794    ]
  1 : [ -4.14787      0.999716    ]
  2 : [ -4.07872      0.999613    ]
  3 : [ -4.00958      0.999477    ]
  4 : [ -3.94043      0.999299    ]
  5 : [ -3.87128      0.999071    ]
  6 : [ -3.80213      0.998781    ]
  7 : [ -3.73298      0.998415    ]
  8 : [ -3.66384      0.997958    ]
  9 : [ -3.59469      0.997394    ]
 10 : [ -3.52554      0.996706    ]
 11 : [ -3.45639      0.995874    ]
 12 : [ -3.38724      0.994879    ]
 13 : [ -3.3181       0.993703    ]
 14 : [ -3.24895      0.992325    ]
 15 : [ -3.1798       0.990729    ]
 16 : [ -3.11065      0.988898    ]
 17 : [ -3.0415       0.986822    ]
 18 : [ -2.97236      0.984491    ]
 19 : [ -2.90321      0.9819      ]
 20 : [ -2.83406      0.979051    ]
 21 : [ -2.76491      0.975948    ]
 22 : [ -2.69577      0.972604    ]
 23 : [ -2.62662      0.969034    ]
 24 : [ -2.55747      0.96526     ]
 25 : [ -2.48832      0.961308    ]
 26 : [ -2.41917      0.957204    ]
 27 : [ -2.35003      0.95298     ]
 28 : [ -2.28088      0.948665    ]
 29 : [ -2.21173      0.944287    ]
 30 : [ -2.14258      0.939871    ]
 31 : [ -2.07343      0.935435    ]
 32 : [ -2.00429      0.930993    ]
 33 : [ -1.93514      0.926547    ]
 34 : [ -1.86599      0.922092    ]
 35 : [ -1.79684      0.917611    ]
 36 : [ -1.72769      0.913076    ]
 37 : [ -1.65855      0.908448    ]
 38 : [ -1.5894       0.903679    ]
 39 : [ -1.52025      0.898708    ]
 40 : [ -1.4511       0.893469    ]
 41 : [ -1.38196      0.887886    ]
 42 : [ -1.31281      0.881881    ]
 43 : [ -1.24366      0.875371    ]
 44 : [ -1.17451      0.868274    ]
 45 : [ -1.10536      0.860509    ]
 46 : [ -1.03622      0.851999    ]
 47 : [ -0.967068     0.842675    ]
 48 : [ -0.89792      0.832473    ]
 49 : [ -0.828772     0.82134     ]
 50 : [ -0.759624     0.809233    ]
 51 : [ -0.690476     0.79612     ]
 52 : [ -0.621328     0.781981    ]
 53 : [ -0.55218      0.766808    ]
 54 : [ -0.483032     0.750604    ]
 55 : [ -0.413885     0.733384    ]
 56 : [ -0.344737     0.715172    ]
 57 : [ -0.275589     0.696006    ]
 58 : [ -0.206441     0.67593     ]
 59 : [ -0.137293     0.654999    ]
 60 : [ -0.068145     0.633277    ]
 61 : [  0.00100292   0.610836    ]
 62 : [  0.0701508    0.587757    ]
 63 : [  0.139299     0.564125    ]
 64 : [  0.208447     0.540037    ]
 65 : [  0.277595     0.515593    ]
 66 : [  0.346742     0.490898    ]
 67 : [  0.41589      0.466064    ]
 68 : [  0.485038     0.441203    ]
 69 : [  0.554186     0.416431    ]
 70 : [  0.623334     0.391863    ]
 71 : [  0.692482     0.36761     ]
 72 : [  0.76163      0.343781    ]
 73 : [  0.830778     0.320479    ]
 74 : [  0.899926     0.297797    ]
 75 : [  0.969074     0.275822    ]
 76 : [  1.03822      0.254628    ]
 77 : [  1.10737      0.234278    ]
 78 : [  1.17652      0.214826    ]
 79 : [  1.24567      0.196311    ]
 80 : [  1.31481      0.178762    ]
 81 : [  1.38396      0.162198    ]
 82 : [  1.45311      0.146625    ]
 83 : [  1.52226      0.132044    ]
 84 : [  1.5914       0.118445    ]
 85 : [  1.66055      0.105814    ]
 86 : [  1.7297       0.094128    ]
 87 : [  1.79885      0.083362    ]
 88 : [  1.868        0.0734859   ]
 89 : [  1.93714      0.0644666   ]
 90 : [  2.00629      0.0562683   ]
 91 : [  2.07544      0.0488529   ]
 92 : [  2.14459      0.0421804   ]
 93 : [  2.21374      0.0362095   ]
 94 : [  2.28288      0.0308972   ]
 95 : [  2.35203      0.0261997   ]
 96 : [  2.42118      0.0220725   ]
 97 : [  2.49033      0.0184707   ]
 98 : [  2.55948      0.0153496   ]
 99 : [  2.62862      0.0126647   ]
100 : [  2.69777      0.0103727   ]
101 : [  2.76692      0.00843133  ]
102 : [  2.83607      0.00680032  ]
103 : [  2.90521      0.00544143  ]
104 : [  2.97436      0.00431891  ]
105 : [  3.04351      0.00339974  ]
106 : [  3.11266      0.00265376  ]
107 : [  3.18181      0.00205382  ]
108 : [  3.25095      0.00157577  ]
109 : [  3.3201       0.0011984   ]
110 : [  3.38925      0.00090331  ]
111 : [  3.4584       0.000674769 ]
112 : [  3.52755      0.000499476 ]
113 : [  3.59669      0.000366333 ]
114 : [  3.66584      0.000266196 ]
115 : [  3.73499      0.000191628 ]
116 : [  3.80414      0.000136653 ]
117 : [  3.87329      9.65267e-05 ]
118 : [  3.94243      6.75337e-05 ]
119 : [  4.01158      4.67965e-05 ]
120 : [  4.08073      3.21145e-05 ]
121 : [  4.14988      2.18254e-05 ]
122 : [  4.21903      1.46885e-05 ]
123 : [  4.28817      9.78877e-06 ]
124 : [  4.35732      6.45945e-06 ]
125 : [  4.42647      4.2205e-06  ]
126 : [  4.49562      2.73034e-06 ]
127 : [  4.56476      1.74881e-06 ]
128 : [  4.63391      1.10899e-06 ],  0 : [ -3.16918      1           ]
  1 : [ -3.12246      1           ]
  2 : [ -3.07575      1           ]
  3 : [ -3.02903      0.999999    ]
  4 : [ -2.98232      0.999999    ]
  5 : [ -2.9356       0.999998    ]
  6 : [ -2.88889      0.999996    ]
  7 : [ -2.84217      0.999994    ]
  8 : [ -2.79546      0.99999     ]
  9 : [ -2.74874      0.999985    ]
 10 : [ -2.70203      0.999976    ]
 11 : [ -2.65531      0.999963    ]
 12 : [ -2.60859      0.999944    ]
 13 : [ -2.56188      0.999916    ]
 14 : [ -2.51516      0.999875    ]
 15 : [ -2.46845      0.999816    ]
 16 : [ -2.42173      0.999733    ]
 17 : [ -2.37502      0.999617    ]
 18 : [ -2.3283       0.999458    ]
 19 : [ -2.28159      0.999241    ]
 20 : [ -2.23487      0.998951    ]
 21 : [ -2.18816      0.998567    ]
 22 : [ -2.14144      0.998065    ]
 23 : [ -2.09472      0.997417    ]
 24 : [ -2.04801      0.996592    ]
 25 : [ -2.00129      0.995552    ]
 26 : [ -1.95458      0.994259    ]
 27 : [ -1.90786      0.992668    ]
 28 : [ -1.86115      0.990735    ]
 29 : [ -1.81443      0.98841     ]
 30 : [ -1.76772      0.985645    ]
 31 : [ -1.721        0.982391    ]
 32 : [ -1.67429      0.9786      ]
 33 : [ -1.62757      0.974227    ]
 34 : [ -1.58086      0.96923     ]
 35 : [ -1.53414      0.96357     ]
 36 : [ -1.48742      0.957216    ]
 37 : [ -1.44071      0.95014     ]
 38 : [ -1.39399      0.942322    ]
 39 : [ -1.34728      0.933749    ]
 40 : [ -1.30056      0.924415    ]
 41 : [ -1.25385      0.914321    ]
 42 : [ -1.20713      0.903479    ]
 43 : [ -1.16042      0.891906    ]
 44 : [ -1.1137       0.879628    ]
 45 : [ -1.06699      0.866678    ]
 46 : [ -1.02027      0.853099    ]
 47 : [ -0.973555     0.838936    ]
 48 : [ -0.926839     0.824244    ]
 49 : [ -0.880124     0.809078    ]
 50 : [ -0.833408     0.793497    ]
 51 : [ -0.786693     0.777556    ]
 52 : [ -0.739977     0.761311    ]
 53 : [ -0.693262     0.744809    ]
 54 : [ -0.646547     0.728089    ]
 55 : [ -0.599831     0.711179    ]
 56 : [ -0.553116     0.694095    ]
 57 : [ -0.5064       0.676836    ]
 58 : [ -0.459685     0.659388    ]
 59 : [ -0.412969     0.641721    ]
 60 : [ -0.366254     0.623794    ]
 61 : [ -0.319539     0.605552    ]
 62 : [ -0.272823     0.586933    ]
 63 : [ -0.226108     0.567873    ]
 64 : [ -0.179392     0.548308    ]
 65 : [ -0.132677     0.528182    ]
 66 : [ -0.0859615    0.50745     ]
 67 : [ -0.0392461    0.486087    ]
 68 : [  0.00746936   0.46409     ]
 69 : [  0.0541848    0.441482    ]
 70 : [  0.1009       0.418313    ]
 71 : [  0.147616     0.394667    ]
 72 : [  0.194331     0.370652    ]
 73 : [  0.241046     0.346405    ]
 74 : [  0.287762     0.322083    ]
 75 : [  0.334477     0.297861    ]
 76 : [  0.381193     0.273922    ]
 77 : [  0.427908     0.250451    ]
 78 : [  0.474624     0.22763     ]
 79 : [  0.521339     0.205629    ]
 80 : [  0.568054     0.184599    ]
 81 : [  0.61477      0.16467     ]
 82 : [  0.661485     0.145948    ]
 83 : [  0.708201     0.12851     ]
 84 : [  0.754916     0.112406    ]
 85 : [  0.801632     0.0976584   ]
 86 : [  0.848347     0.0842659   ]
 87 : [  0.895062     0.0722042   ]
 88 : [  0.941778     0.0614301   ]
 89 : [  0.988493     0.0518849   ]
 90 : [  1.03521      0.0434974   ]
 91 : [  1.08192      0.0361879   ]
 92 : [  1.12864      0.0298709   ]
 93 : [  1.17536      0.0244578   ]
 94 : [  1.22207      0.0198592   ]
 95 : [  1.26879      0.0159871   ]
 96 : [  1.3155       0.0127563   ]
 97 : [  1.36222      0.0100858   ]
 98 : [  1.40893      0.00789959  ]
 99 : [  1.45565      0.00612761  ]
100 : [  1.50236      0.00470602  ]
101 : [  1.54908      0.00357749  ]
102 : [  1.59579      0.00269128  ]
103 : [  1.64251      0.00200304  ]
104 : [  1.68922      0.0014746   ]
105 : [  1.73594      0.00107353  ]
106 : [  1.78266      0.000772735 ]
107 : [  1.82937      0.00054984  ]
108 : [  1.87609      0.000386686 ]
109 : [  1.9228       0.000268737 ]
110 : [  1.96952      0.000184535 ]
111 : [  2.01623      0.000125185 ]
112 : [  2.06295      8.38873e-05 ]
113 : [  2.10966      5.55213e-05 ]
114 : [  2.15638      3.62909e-05 ]
115 : [  2.20309      2.34244e-05 ]
116 : [  2.24981      1.49292e-05 ]
117 : [  2.29653      9.39432e-06 ]
118 : [  2.34324      5.83612e-06 ]
119 : [  2.38996      3.57919e-06 ]
120 : [  2.43667      2.16681e-06 ]
121 : [  2.48339      1.29482e-06 ]
122 : [  2.5301       7.63709e-07 ]
123 : [  2.57682      4.44587e-07 ]
124 : [  2.62353      2.55434e-07 ]
125 : [  2.67025      1.44836e-07 ]
126 : [  2.71696      8.10465e-08 ]
127 : [  2.76368      4.47547e-08 ]
128 : [  2.81039      2.43881e-08 ]]
outliers= [[-2.29006],[]]
class=DataModel name=myDataModel2 fileName=data.csv inputColumns=[0,2] outputColumns=[1] inputNames=[var1,var2] outputNames=[var3]
inputNames= [var1,var2]
outputNames= [var3]
min= [0 : [ 1.20548  0.350042 ]]
max= [0 : [ -0.355007  0.810668 ]]
class=DataModel name=myDataModel3 fileName=data.csv inputColumns=[0,2] outputColumns=[1] inputNames=[X0,X2] outputNames=[X1]
inputNames= [X0,X2]
outputNames= [X1]
inputSample=     [ X0         X2         ]
0 : [  0.608202  -0.438266  ]
1 : [  1.20548    0.350042  ]
2 : [ -0.355007   0.810668  ]
3 : [  0.793156   0.261018  ]
4 : [ -2.29006   -1.31178   ]
5 : [ -0.0907838 -0.139453  ]
6 : [ -0.560206   0.322925  ]
7 : [  0.445785  -0.856712  ]
8 : [  0.473617   0.351418  ]
9 : [  1.78236   -0.781366  ]
outputSample=     [ X1         ]
0 : [ -1.26617   ]
1 : [ -2.18139   ]
2 : [  1.43725   ]
3 : [ -0.470526  ]
4 : [ -1.28289   ]
5 : [  0.995793  ]
6 : [  0.44549   ]
7 : [ -1.03808   ]
8 : [ -0.125498  ]
9 : [  0.0702074 ]
min= [0 : [ 1.20548  0.350042 ]]
max= [0 : [ -0.355007  0.810668 ]]
class=DataModel name=myDataModel4 fileName=data_500.csv inputColumns=[0,1,2] outputColumns=[] inputNames=[X0,X1,X2] outputNames=[]
inputNames= [X0,X1,X2]
outputNames= []
class=QuantileAnalysisResult quantiles=X0: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.46389,-2.23369,-2.0035],[-2.51804,-2.4067,-2.29536]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[1.92083,2.30459,2.68835],[2.85639,3.15958,3.46277]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.47455,-2.29006,-2.10558],[-2.48545,-2.4067,-2.32795]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[2.41487,3.02799,3.64111],[2.94514,3.15958,3.37402]]]X1: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=1 dimension=3 data=[[-3.36709,-2.84159,-2.31609]]]X2: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[-1.73368,-1.5705,-1.40732],[-3.6264,-2.98535,-2.3443],[-3.32293,-3.09834,-2.87375]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[1.51902,1.69843,1.87784],[2.29461,2.5038,2.71299],[2.53902,2.63821,2.7374]]] validity= (X0,1): [1,0] (X0,2): [1,0] (X0,4): [1,0] (X1,1): [1] (X2,4): [1,1,0]
InvalidArgumentException : The maximum target probability for X2 is not suitable with the threshold of the lower tail : it should be lower than 0.05
class=QuantileAnalysisResult quantiles=X0: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.27711,-2.17544,-1.90984],[-2.49332,-2.39792,-1.96097]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[1.9623,2.3813,2.64831],[2.0624,3.26237,4.05574]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.35418,-2.27964,-2.01368],[-2.55356,-2.42104,-2.07761]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[2.183,2.68119,3.0453],[2.29596,3.47036,4.6793]]]X1: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=1 dimension=3 data=[[-2.99516,-2.71201,-2.24854]]]X2: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[-2.55393,-2.29831,-1.94145],[-2.92768,-2.59746,-2.00007],[-4.94661,-3.53311,-2.03694]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[2.11894,2.37494,2.48616],[2.17227,2.49117,2.56235],[2.20817,2.63044,2.68046]]] validity= (X0,1): 0.048123 (X0,2): 0.837788 (X1,1): 0.859301 (X2,1): 0.368751 (X2,2): 0.355047
    [ X0    X1    X2    ]
0 : [ 0.114 0.12  0.106 ]
1 : [ 0.924 0.89  0.892 ]
class=QuantileAnalysisResult quantiles=X0: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.25066,-2.13742,-1.97106],[-2.63007,-2.42975,-2.00384]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[2.0336,2.39169,2.66636],[2.10793,3.24011,3.91598]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.40182,-2.26008,-2.10424],[-2.72218,-2.47166,-2.13712]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[2.29473,2.68407,3.05917],[2.35091,3.43519,4.37443]]]X1: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=1 dimension=3 data=[[-3.20317,-2.67809,-2.29305]]]X2: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[-2.5951,-2.33329,-2.05612],[-2.90863,-2.60824,-2.09622],[-4.16267,-3.37042,-2.1222]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[2.1073,2.30144,2.43061],[2.14865,2.43789,2.55157],[2.17072,2.67384,2.85496]]] validity= (X0,1): 0.633769 (X0,2): 0.847489 (X1,1): 0.957051 (X2,1): 0.556529 (X2,2): 0.40933
    [ X0    X2    ]
0 : [ 0.114 0.106 ]
1 : [ 0.924 0.892 ]
class=QuantileAnalysisResult quantiles=X0: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.25066,-2.13742,-1.97106],[-2.63007,-2.42975,-2.00384]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[2.0336,2.39169,2.66636],[2.10793,3.24011,3.91598]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[-2.40182,-2.26008,-2.10424],[-2.72218,-2.47166,-2.13712]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=2 dimension=3 data=[[2.29473,2.68407,3.05917],[2.35091,3.43519,4.37443]]]X2: [class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[-2.5951,-2.33329,-2.05612],[-2.90863,-2.60824,-2.09622],[-4.16267,-3.37042,-2.1222]],class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=3 dimension=3 data=[[2.1073,2.30144,2.43061],[2.14865,2.43789,2.55157],[2.17072,2.67384,2.85496]]] validity= (X0,1): 0.633769 (X0,2): 0.847489 (X2,1): 0.556529 (X2,2): 0.40933
#!/usr/bin/env python

import openturns as ot
import persalys

myStudy = persalys.Study('myStudy')
persalys.Study.Add(myStudy)
inputColumns = [0, 2]
outputColumns = []
inputNames = ['X0', 'X2']
outputNames = []
myDataModel = persalys.DataModel('myDataModel', 'data.csv', inputColumns, outputColumns, inputNames, outputNames)
myStudy.add(myDataModel)
inputColumns = [0, 2]
outputColumns = [1]
inputNames = ['var1', 'var2']
outputNames = ['var3']
myDataModel2 = persalys.DataModel('myDataModel2', 'data.csv', inputColumns, outputColumns, inputNames, outputNames)
myStudy.add(myDataModel2)
inputColumns = [0, 2]
outputColumns = [1]
inputNames = ['X0', 'X2']
outputNames = ['X1']
myDataModel3 = persalys.DataModel('myDataModel3', 'data.csv', inputColumns, outputColumns, inputNames, outputNames)
myStudy.add(myDataModel3)
inputColumns = [0, 1, 2]
outputColumns = []
inputNames = ['X0', 'X1', 'X2']
outputNames = []
myDataModel4 = persalys.DataModel('myDataModel4', 'data_500.csv', inputColumns, outputColumns, inputNames, outputNames)
myStudy.add(myDataModel4)
aDataAnalysis = persalys.DataAnalysis('aDataAnalysis', myDataModel)
myStudy.add(aDataAnalysis)
aQuantileAnalysis = persalys.QuantileAnalysis('aQuantileAnalysis', myDataModel4)
aQuantileAnalysis.setInterestVariables(['X0', 'X2'])
aQuantileAnalysis.setTargetProbabilities([[0.01, 0.001],[0.02, 0.01, 0.001]])
aQuantileAnalysis.setTailTypes([7, 4])
aQuantileAnalysis.setType(1)
aQuantileAnalysis.setThreshold([[-1.2,-1.2],
[1.3,1.3]]
)
myStudy.add(aQuantileAnalysis)

